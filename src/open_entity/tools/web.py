# -*- coding: utf-8 -*-
"""
Webé–¢é€£ãƒ„ãƒ¼ãƒ« - Gemini Grounding ãƒ™ãƒ¼ã‚¹

BeautifulSoup ã‚„ Google Custom Search API ã‚’ä½¿ã‚ãšã€
Gemini ã® Google Search Grounding æ©Ÿèƒ½ã§ Web æ¤œç´¢ã‚’å®Ÿè¡Œ
"""
import html
import os
import re
import socket
import ipaddress
import urllib.parse
import urllib.request
from html.parser import HTMLParser
from typing import List, Optional

from open_entity.core.llm_provider import generate_text, get_preferred_provider, get_analyzer_model

try:
    from google import genai
    from google.genai import types
    HAS_GENAI = True
except ImportError:
    genai = None
    types = None
    HAS_GENAI = False

USER_AGENT = "Mozilla/5.0 (compatible; open-entity/1.0; +https://open-entity)"
MAX_RESULTS = 5
MAX_FETCH_CHARS = 12000
MAX_FETCH_BYTES = 2 * 1024 * 1024


def _has_gemini_key() -> bool:
    return bool(
        os.getenv("GENAI_API_KEY") or os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    )


def _is_private_url(url: str) -> bool:
    """URLãŒãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆIP/localhostã‚’æŒ‡ã—ã¦ã„ã‚‹ã‹åˆ¤å®šï¼ˆSSRFå¯¾ç­–ï¼‰"""
    try:
        parsed = urllib.parse.urlparse(url)
        hostname = parsed.hostname
        if not hostname:
            return True
        if hostname in ("localhost", "127.0.0.1", "::1"):
            return True
        try:
            ip = ipaddress.ip_address(hostname)
            return ip.is_private or ip.is_loopback or ip.is_reserved
        except ValueError:
            try:
                ip_str = socket.gethostbyname(hostname)
                ip = ipaddress.ip_address(ip_str)
                return ip.is_private or ip.is_loopback or ip.is_reserved
            except socket.gaierror:
                return False
    except Exception:
        return True


def _is_http_url(url: str) -> bool:
    parsed = urllib.parse.urlparse(url)
    return parsed.scheme in ("http", "https")


class _SafeRedirectHandler(urllib.request.HTTPRedirectHandler):
    def redirect_request(self, req, fp, code, msg, headers, newurl):
        if not newurl:
            return None
        if not _is_http_url(newurl):
            raise ValueError("Unsupported URL scheme")
        if _is_private_url(newurl):
            raise ValueError("Access to private/internal URLs is not allowed")
        return super().redirect_request(req, fp, code, msg, headers, newurl)


def _fetch_url(url: str, timeout: int = 20, max_bytes: int = MAX_FETCH_BYTES) -> tuple[str, str, bool]:
    """URLã‚’å–å¾—ã—ã¦ (text, content_type, truncated) ã‚’è¿”ã™ã€‚"""
    if not _is_http_url(url):
        raise ValueError("Only http/https URLs are supported")
    if _is_private_url(url):
        raise ValueError("Access to private/internal URLs is not allowed")

    req = urllib.request.Request(url, headers={"User-Agent": USER_AGENT})
    opener = urllib.request.build_opener(_SafeRedirectHandler())
    with opener.open(req, timeout=timeout) as response:
        content_type = response.headers.get("Content-Type", "")
        chunks = []
        total = 0
        truncated = False
        while True:
            read_size = min(65536, max_bytes - total)
            if read_size <= 0:
                truncated = True
                break
            chunk = response.read(read_size)
            if not chunk:
                break
            chunks.append(chunk)
            total += len(chunk)
            if total >= max_bytes:
                truncated = True
                break
        raw = b"".join(chunks)

    charset_match = re.search(r"charset=([^;]+)", content_type, re.IGNORECASE)
    charset = charset_match.group(1).strip() if charset_match else "utf-8"
    text = raw.decode(charset, errors="ignore")
    return text, content_type, truncated


def _strip_html(html_text: str) -> str:
    """HTMLã‚’ç°¡æ˜“çš„ã«ãƒ†ã‚­ã‚¹ãƒˆåŒ–"""
    text = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", html_text)
    text = re.sub(r"(?s)<[^>]+>", " ", text)
    text = html.unescape(text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def _decode_ddg_url(url: str) -> str:
    """DuckDuckGoã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‚’å¾©å·"""
    if "duckduckgo.com/l/?" in url:
        parsed = urllib.parse.urlparse(url)
        qs = urllib.parse.parse_qs(parsed.query)
        if "uddg" in qs and qs["uddg"]:
            return urllib.parse.unquote(qs["uddg"][0])
    if url.startswith("//"):
        return "https:" + url
    return url


class _DuckDuckGoParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.results: List[dict] = []
        self._current = None
        self._in_title = False
        self._in_snippet = False

    def handle_starttag(self, tag, attrs):
        attrs = dict(attrs)
        if tag == "a" and "class" in attrs and "result__a" in attrs["class"]:
            if self._current:
                self.results.append(self._current)
            self._current = {
                "title": "",
                "url": _decode_ddg_url(attrs.get("href", "")),
                "snippet": "",
            }
            self._in_title = True
        elif tag in ("a", "div") and "class" in attrs and "result__snippet" in attrs["class"]:
            if self._current:
                self._in_snippet = True

    def handle_endtag(self, tag):
        if tag == "a" and self._in_title:
            self._in_title = False
        if tag in ("a", "div") and self._in_snippet:
            self._in_snippet = False

    def handle_data(self, data):
        if self._in_title and self._current is not None:
            self._current["title"] += data
        elif self._in_snippet and self._current is not None:
            self._current["snippet"] += data

    def close(self):
        super().close()
        if self._current:
            self.results.append(self._current)
            self._current = None


def _search_duckduckgo(query: str, max_results: int = MAX_RESULTS) -> List[dict]:
    url = "https://duckduckgo.com/html/?" + urllib.parse.urlencode({"q": query})
    html_text, _, _ = _fetch_url(url)
    parser = _DuckDuckGoParser()
    parser.feed(html_text)
    parser.close()
    results = []
    seen = set()
    for item in parser.results:
        title = (item.get("title") or "").strip()
        url = (item.get("url") or "").strip()
        if not title or not url:
            continue
        if url in seen:
            continue
        seen.add(url)
        results.append({
            "title": title,
            "url": url,
            "snippet": (item.get("snippet") or "").strip(),
        })
        if len(results) >= max_results:
            break
    return results


def _summarize_search_results(query: str, results: List[dict]) -> str:
    sources = []
    for idx, r in enumerate(results, 1):
        snippet = r.get("snippet", "")
        sources.append(
            f"[{idx}] {r.get('title','')}\nURL: {r.get('url','')}\nSnippet: {snippet}"
        )
    sources_text = "\n\n".join(sources)

    prompt = f"""æ¬¡ã®æ¤œç´¢çµæœã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
æƒ…å ±ãŒä¸è¶³ã™ã‚‹å ´åˆã¯ã€Œä¸æ˜ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
å¯èƒ½ãªã‚‰å›ç­”ä¸­ã« [1] [2] ã®ã‚ˆã†ãªå‚ç…§ç•ªå·ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚

è³ªå•:
{query}

æ¤œç´¢çµæœ:
{sources_text}
"""
    provider = get_preferred_provider()
    model = get_analyzer_model(provider)
    return generate_text(prompt=prompt, provider=provider, model=model, temperature=0.2, max_tokens=800)


def websearch(query: str, site_filter: Optional[str] = None) -> str:
    """
    Gemini ã® Google Search Grounding ã‚’ä½¿ç”¨ã—ã¦ Web æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        site_filter: æ¤œç´¢ã‚’åˆ¶é™ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ (ä¾‹: "nta.go.jp")

    Returns:
        æ¤œç´¢çµæœã‚’å«ã‚€å›ç­”ï¼ˆå‚ç…§å…ƒ URL ä»˜ãï¼‰
    """
    use_gemini = HAS_GENAI and _has_gemini_key()

    # ã‚µã‚¤ãƒˆåˆ¶é™ãŒã‚ã‚‹å ´åˆã€ã‚¯ã‚¨ãƒªã‚’ä¿®æ­£
    search_query = query
    if site_filter:
        search_query = f"site:{site_filter} {query}"

    if use_gemini:
        api_key = os.getenv("GENAI_API_KEY") or os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        try:
            client = genai.Client(api_key=api_key)

            # Google Search Grounding ã‚’æœ‰åŠ¹ã«ã—ã¦ç”Ÿæˆ
            response = client.models.generate_content(
                model='gemini-2.0-flash',
                contents=search_query,
                config=types.GenerateContentConfig(
                    tools=[types.Tool(google_search=types.GoogleSearch())]
                )
            )

            result_parts = []

            # ãƒ¡ã‚¤ãƒ³å›ç­”
            result_parts.append(response.text)

            # å‚ç…§å…ƒ URL ã‚’æŠ½å‡º
            sources = _extract_grounding_sources(response)
            if sources:
                result_parts.append("\n\nğŸ“š å‚ç…§å…ƒ:")
                for source in sources[:MAX_RESULTS]:
                    result_parts.append(f"  - {source['title']}: {source['url']}")

            return "\n".join(result_parts)

        except Exception as e:
            return f"Error: Webæ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ (Gemini): {e}"

    # Fallback: DuckDuckGo HTML + LLM summary
    try:
        results = _search_duckduckgo(search_query, max_results=MAX_RESULTS)
        if not results:
            return "Error: æ¤œç´¢çµæœãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        try:
            answer = _summarize_search_results(query, results)
            sources = "\n".join([f"  - {r['title']}: {r['url']}" for r in results])
            return f"{answer}\n\nğŸ“š å‚ç…§å…ƒ:\n{sources}"
        except Exception:
            sources = "\n".join([f"  - {r['title']}: {r['url']}" for r in results])
            return f"æ¤œç´¢çµæœï¼ˆè¦ç´„ãªã—ï¼‰:\n{sources}"
    except Exception as e:
        return f"Error: Webæ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ (fallback): {e}"


def _extract_grounding_sources(response) -> List[dict]:
    """Grounding ã®ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æŠ½å‡º"""
    sources = []

    try:
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                metadata = candidate.grounding_metadata
                if hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                    for chunk in metadata.grounding_chunks:
                        if hasattr(chunk, 'web') and chunk.web:
                            # Vertex AI ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ URL ã‹ã‚‰å®Ÿéš›ã® URL ã‚’å–å¾—ã™ã‚‹ã®ã¯é›£ã—ã„ã®ã§ã€
                            # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ URL ã‚’ãã®ã¾ã¾ä½¿ç”¨
                            sources.append({
                                'title': chunk.web.title if hasattr(chunk.web, 'title') else 'Unknown',
                                'url': chunk.web.uri if hasattr(chunk.web, 'uri') else ''
                            })
    except Exception:
        pass  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¦ã‚‚å›ç­”ã¯è¿”ã™

    return sources


def webfetch(url: str, question: Optional[str] = None) -> str:
    """
    æŒ‡å®šã—ãŸ URL ã®å†…å®¹ã‚’ Gemini ã§è¦ç´„ã—ã¦å–å¾—ã—ã¾ã™ã€‚

    Args:
        url: å–å¾—ã™ã‚‹ URL
        question: URL ã®å†…å®¹ã«å¯¾ã™ã‚‹è³ªå•ï¼ˆçœç•¥æ™‚ã¯è¦ç´„ï¼‰

    Returns:
        URL ã®å†…å®¹ã®è¦ç´„ã¾ãŸã¯è³ªå•ã¸ã®å›ç­”
    """
    use_gemini = HAS_GENAI and _has_gemini_key()

    if use_gemini:
        api_key = os.getenv("GENAI_API_KEY") or os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        prompt = question or "ã“ã® URL ã®å†…å®¹ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
        full_prompt = f"ä»¥ä¸‹ã® URL ã®å†…å®¹ã«ã¤ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nURL: {url}\n\nè³ªå•: {prompt}"

        try:
            client = genai.Client(api_key=api_key)
            response = client.models.generate_content(
                model='gemini-2.0-flash',
                contents=full_prompt,
                config=types.GenerateContentConfig(
                    tools=[types.Tool(google_search=types.GoogleSearch())]
                )
            )
            return f"URL: {url}\n\n{response.text}"
        except Exception as e:
            return f"Error: URL ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ (Gemini): {e}"

    if _is_private_url(url):
        return "Error: Access to private/internal URLs is not allowed"

    prompt = question or "ã“ã® URL ã®å†…å®¹ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
    try:
        text, content_type, truncated = _fetch_url(url)
        if "text/html" in content_type.lower():
            text = _strip_html(text)
        if len(text) > MAX_FETCH_CHARS:
            text = text[:MAX_FETCH_CHARS] + "... [TRUNCATED]"
        elif truncated:
            text = text + "... [TRUNCATED]"

        provider = get_preferred_provider()
        model = get_analyzer_model(provider)
        full_prompt = f"""ä»¥ä¸‹ã®URLå†…å®¹ã‹ã‚‰è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
URL: {url}
è³ªå•: {prompt}

æœ¬æ–‡ï¼ˆæŠœç²‹ï¼‰:
{text}
"""
        try:
            answer = generate_text(prompt=full_prompt, provider=provider, model=model, temperature=0.2, max_tokens=800)
            return f"URL: {url}\n\n{answer}"
        except Exception:
            return f"URL: {url}\n\næœ¬æ–‡ï¼ˆæŠœç²‹ï¼‰:\n{text}"
    except Exception as e:
        return f"Error: URL ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ (fallback): {e}"
