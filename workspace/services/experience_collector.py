"""Experience Collector Module

ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœã‚’è¨˜éŒ²ã—ã€æˆåŠŸ/å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚
EntityMemoryã®MemoryType.EXPERIENCEã‚’ä½¿ç”¨ã—ã¦çµŒé¨“ã‚’è“„ç©ã™ã‚‹ã€‚
"""

import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import hashlib

from services.entity_memory import EntityMemory, MemoryType, ImportanceLevel, get_memory


class TaskResult(Enum):
    """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœ"""
    SUCCESS = "success"
    FAILURE = "failure"
    PARTIAL = "partial"  # éƒ¨åˆ†çš„æˆåŠŸ
    TIMEOUT = "timeout"
    CANCELLED = "cancelled"


@dataclass
class TaskExecutionRecord:
    """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œè¨˜éŒ²"""
    task_id: str
    task_type: str  # ã‚¿ã‚¹ã‚¯ã®ç¨®é¡ï¼ˆã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€èª¿æŸ»ã€etc.ï¼‰
    result: TaskResult
    duration_seconds: float
    resources: Dict[str, Any]  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€APIã‚³ãƒ¼ãƒ«å›æ•°ç­‰
    error_message: Optional[str] = None
    retry_count: int = 0
    context: Dict[str, Any] = None  # è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()
        if self.context is None:
            self.context = {}
    
    def to_dict(self) -> Dict:
        return {
            "task_id": self.task_id,
            "task_type": self.task_type,
            "result": self.result.value,
            "duration_seconds": self.duration_seconds,
            "resources": self.resources,
            "error_message": self.error_message,
            "retry_count": self.retry_count,
            "context": self.context,
            "timestamp": self.timestamp.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> "TaskExecutionRecord":
        return cls(
            task_id=data["task_id"],
            task_type=data["task_type"],
            result=TaskResult(data["result"]),
            duration_seconds=data["duration_seconds"],
            resources=data["resources"],
            error_message=data.get("error_message"),
            retry_count=data.get("retry_count", 0),
            context=data.get("context", {}),
            timestamp=datetime.fromisoformat(data["timestamp"])
        )


@dataclass
class SuccessPattern:
    """æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³"""
    pattern_id: str
    task_type: str
    avg_duration: float
    common_resources: Dict[str, Any]
    success_count: int
    common_tags: List[str]
    last_success: datetime
    
    def to_dict(self) -> Dict:
        return {
            "pattern_id": self.pattern_id,
            "task_type": self.task_type,
            "avg_duration": self.avg_duration,
            "common_resources": self.common_resources,
            "success_count": self.success_count,
            "common_tags": self.common_tags,
            "last_success": self.last_success.isoformat()
        }


@dataclass
class FailureAnalysis:
    """å¤±æ•—åˆ†æ"""
    analysis_id: str
    task_type: str
    failure_count: int
    common_errors: List[Dict[str, Any]]  # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨å›æ•°
    avg_duration_before_failure: float
    common_resource_patterns: Dict[str, Any]
    recommended_actions: List[str]
    last_failure: datetime
    
    def to_dict(self) -> Dict:
        return {
            "analysis_id": self.analysis_id,
            "task_type": self.task_type,
            "failure_count": self.failure_count,
            "common_errors": self.common_errors,
            "avg_duration_before_failure": self.avg_duration_before_failure,
            "common_resource_patterns": self.common_resource_patterns,
            "recommended_actions": self.recommended_actions,
            "last_failure": self.last_failure.isoformat()
        }


class ExperienceCollector:
    """çµŒé¨“åé›†ãƒ»åˆ†æã‚¯ãƒ©ã‚¹
    
    ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœã‚’è¨˜éŒ²ã—ã€æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡ºã¨å¤±æ•—åˆ†æã‚’è¡Œã†ã€‚
    EntityMemoryã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã€‚
    """
    
    def __init__(self, memory: Optional[EntityMemory] = None):
        """
        Args:
            memory: EntityMemoryã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆNoneã®å ´åˆã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨ï¼‰
        """
        self.memory = memory or get_memory()
        self._local_cache: List[TaskExecutionRecord] = []  # æœ€è¿‘ã®è¨˜éŒ²ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._max_cache_size = 1000
    
    def record_task_execution(
        self,
        task_id: str,
        result: TaskResult,
        duration: float,
        resources: Dict[str, Any],
        task_type: str = "general",
        error_message: Optional[str] = None,
        retry_count: int = 0,
        context: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None
    ) -> str:
        """
        ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµæœã‚’è¨˜éŒ²
        
        Args:
            task_id: ã‚¿ã‚¹ã‚¯ID
            result: å®Ÿè¡Œçµæœï¼ˆSUCCESS/FAILURE/PARTIAL/TIMEOUT/CANCELLEDï¼‰
            duration: å®Ÿè¡Œæ™‚é–“ï¼ˆç§’ï¼‰
            resources: ä½¿ç”¨ãƒªã‚½ãƒ¼ã‚¹ï¼ˆmemory_mb, api_calls, tokensç­‰ï¼‰
            task_type: ã‚¿ã‚¹ã‚¯ç¨®åˆ¥
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆå¤±æ•—æ™‚ï¼‰
            retry_count: ãƒªãƒˆãƒ©ã‚¤å›æ•°
            context: è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            tags: ã‚¿ã‚°ãƒªã‚¹ãƒˆ
        
        Returns:
            è¨˜éŒ²ID
        """
        record = TaskExecutionRecord(
            task_id=task_id,
            task_type=task_type,
            result=result,
            duration_seconds=duration,
            resources=resources,
            error_message=error_message,
            retry_count=retry_count,
            context=context or {}
        )
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ 
        self._local_cache.append(record)
        if len(self._local_cache) > self._max_cache_size:
            self._local_cache.pop(0)
        
        # é‡è¦åº¦ã‚’çµæœã«å¿œã˜ã¦è¨­å®š
        importance = self._determine_importance(result, retry_count)
        
        # ã‚¿ã‚°ç”Ÿæˆ
        auto_tags = self._generate_tags(record)
        if tags:
            auto_tags.extend(tags)
        
        # EntityMemoryã«ä¿å­˜
        content = self._format_record_content(record)
        memory_id = self.memory.store(
            content=content,
            memory_type=MemoryType.EXPERIENCE,
            importance=importance,
            tags=list(set(auto_tags)),  # é‡è¤‡é™¤å»
            context=record.to_dict()
        )
        
        return memory_id
    
    def _determine_importance(self, result: TaskResult, retry_count: int) -> ImportanceLevel:
        """çµæœã«åŸºã¥ã„ã¦é‡è¦åº¦ã‚’æ±ºå®š"""
        if result == TaskResult.FAILURE:
            return ImportanceLevel.HIGH if retry_count == 0 else ImportanceLevel.CRITICAL
        elif result == TaskResult.PARTIAL:
            return ImportanceLevel.MEDIUM
        elif result == TaskResult.TIMEOUT:
            return ImportanceLevel.HIGH
        elif retry_count > 0:
            return ImportanceLevel.HIGH  # ãƒªãƒˆãƒ©ã‚¤ãŒå¿…è¦ã ã£ãŸæˆåŠŸã‚‚é‡è¦
        else:
            return ImportanceLevel.MEDIUM
    
    def _generate_tags(self, record: TaskExecutionRecord) -> List[str]:
        """è¨˜éŒ²ã‹ã‚‰ã‚¿ã‚°ã‚’è‡ªå‹•ç”Ÿæˆ"""
        tags = [
            f"task_type:{record.task_type}",
            f"result:{record.result.value}",
        ]
        
        # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³ã«åŸºã¥ãã‚¿ã‚°
        if record.resources.get("memory_mb", 0) > 500:
            tags.append("high_memory")
        if record.resources.get("api_calls", 0) > 10:
            tags.append("high_api_usage")
        
        # å®Ÿè¡Œæ™‚é–“ã«åŸºã¥ãã‚¿ã‚°
        if record.duration_seconds > 60:
            tags.append("long_running")
        elif record.duration_seconds < 1:
            tags.append("fast_execution")
        
        return tags
    
    def _format_record_content(self, record: TaskExecutionRecord) -> str:
        """è¨˜éŒ²å†…å®¹ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        status_emoji = {
            TaskResult.SUCCESS: "âœ…",
            TaskResult.FAILURE: "âŒ",
            TaskResult.PARTIAL: "âš ï¸",
            TaskResult.TIMEOUT: "â±ï¸",
            TaskResult.CANCELLED: "ğŸš«"
        }
        
        content_parts = [
            f"{status_emoji.get(record.result, 'â“')} Task {record.task_id} ({record.task_type})",
            f"Result: {record.result.value}",
            f"Duration: {record.duration_seconds:.2f}s",
            f"Resources: {json.dumps(record.resources, ensure_ascii=False)}"
        ]
        
        if record.error_message:
            content_parts.append(f"Error: {record.error_message[:200]}")
        
        return " | ".join(content_parts)
    
    def get_success_patterns(
        self,
        task_type: Optional[str] = None,
        min_success_count: int = 3,
        days_back: int = 30
    ) -> List[SuccessPattern]:
        """
        æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        
        Args:
            task_type: ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ç¨®åˆ¥ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆNoneã®å ´åˆå…¨ç¨®åˆ¥ï¼‰
            min_success_count: æœ€å°æˆåŠŸå›æ•°
            days_back: é¡ã‚‹æ—¥æ•°
        
        Returns:
            æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ãƒˆ
        """
        # EntityMemoryã‹ã‚‰æˆåŠŸè¨˜éŒ²ã‚’æ¤œç´¢
        since = datetime.now() - timedelta(days=days_back)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ãƒ¡ãƒ¢ãƒªã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†
        records = self._get_records_in_period(since)
        
        # æˆåŠŸè¨˜éŒ²ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        success_records = [
            r for r in records 
            if r.result == TaskResult.SUCCESS and 
            (task_type is None or r.task_type == task_type)
        ]
        
        # ã‚¿ã‚¹ã‚¯ç¨®åˆ¥ã”ã¨ã«é›†è¨ˆ
        patterns_by_type: Dict[str, List[TaskExecutionRecord]] = {}
        for record in success_records:
            if record.task_type not in patterns_by_type:
                patterns_by_type[record.task_type] = []
            patterns_by_type[record.task_type].append(record)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ
        patterns = []
        for ttype, type_records in patterns_by_type.items():
            if len(type_records) >= min_success_count:
                pattern = self._analyze_success_pattern(ttype, type_records)
                patterns.append(pattern)
        
        # æˆåŠŸå›æ•°ã§ã‚½ãƒ¼ãƒˆ
        patterns.sort(key=lambda p: p.success_count, reverse=True)
        return patterns
    
    def _get_records_in_period(self, since: datetime) -> List[TaskExecutionRecord]:
        """æŒ‡å®šæœŸé–“å†…ã®è¨˜éŒ²ã‚’å–å¾—"""
        records = []
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
        for record in self._local_cache:
            if record.timestamp >= since:
                records.append(record)
        
        # EntityMemoryã‹ã‚‰ã‚‚æ¤œç´¢ï¼ˆEXPERIENCEã‚¿ã‚¤ãƒ—ï¼‰
        memory_entries = self.memory.recall(
            query="Task",
            memory_type=MemoryType.EXPERIENCE,
            limit=1000,
            include_expired=False
        )
        
        for entry in memory_entries:
            if entry.created_at >= since:
                try:
                    context = entry.context
                    if context and "task_id" in context:
                        record = TaskExecutionRecord.from_dict(context)
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„ã‚‚ã®ã ã‘è¿½åŠ 
                        if not any(r.task_id == record.task_id for r in records):
                            records.append(record)
                except (KeyError, ValueError):
                    continue
        
        return records
    
    def _analyze_success_pattern(
        self, 
        task_type: str, 
        records: List[TaskExecutionRecord]
    ) -> SuccessPattern:
        """æˆåŠŸè¨˜éŒ²ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        # å¹³å‡å®Ÿè¡Œæ™‚é–“
        avg_duration = sum(r.duration_seconds for r in records) / len(records)
        
        # å…±é€šãƒªã‚½ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        common_resources = self._extract_common_resources(records)
        
        # å…±é€šã‚¿ã‚°
        all_tags = []
        for r in records:
            all_tags.extend(self._generate_tags(r))
        tag_counts = {}
        for tag in all_tags:
            tag_counts[tag] = tag_counts.get(tag, 0) + 1
        common_tags = [tag for tag, count in tag_counts.items() if count >= len(records) * 0.5]
        
        # æœ€çµ‚æˆåŠŸæ—¥æ™‚
        last_success = max(r.timestamp for r in records)
        
        return SuccessPattern(
            pattern_id=hashlib.md5(f"{task_type}:success".encode()).hexdigest()[:12],
            task_type=task_type,
            avg_duration=avg_duration,
            common_resources=common_resources,
            success_count=len(records),
            common_tags=common_tags,
            last_success=last_success
        )
    
    def _extract_common_resources(self, records: List[TaskExecutionRecord]) -> Dict[str, Any]:
        """å…±é€šãƒªã‚½ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
        if not records:
            return {}
        
        # æ•°å€¤ãƒªã‚½ãƒ¼ã‚¹ã®å¹³å‡å€¤ã‚’è¨ˆç®—
        numeric_keys = ["memory_mb", "api_calls", "tokens", "cpu_percent"]
        common = {}
        
        for key in numeric_keys:
            values = [r.resources.get(key) for r in records if key in r.resources]
            if values:
                common[f"avg_{key}"] = sum(values) / len(values)
                common[f"max_{key}"] = max(values)
                common[f"min_{key}"] = min(values)
        
        return common
    
    def get_failure_analysis(
        self,
        task_type: Optional[str] = None,
        days_back: int = 30
    ) -> List[FailureAnalysis]:
        """
        å¤±æ•—åˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            task_type: ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ç¨®åˆ¥ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆNoneã®å ´åˆå…¨ç¨®åˆ¥ï¼‰
            days_back: é¡ã‚‹æ—¥æ•°
        
        Returns:
            å¤±æ•—åˆ†æãƒªã‚¹ãƒˆ
        """
        since = datetime.now() - timedelta(days=days_back)
        records = self._get_records_in_period(since)
        
        # å¤±æ•—è¨˜éŒ²ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
        failure_records = [
            r for r in records 
            if r.result in [TaskResult.FAILURE, TaskResult.TIMEOUT] and 
            (task_type is None or r.task_type == task_type)
        ]
        
        # ã‚¿ã‚¹ã‚¯ç¨®åˆ¥ã”ã¨ã«é›†è¨ˆ
        failures_by_type: Dict[str, List[TaskExecutionRecord]] = {}
        for record in failure_records:
            if record.task_type not in failures_by_type:
                failures_by_type[record.task_type] = []
            failures_by_type[record.task_type].append(record)
        
        # åˆ†æç”Ÿæˆ
        analyses = []
        for ttype, type_records in failures_by_type.items():
            analysis = self._analyze_failures(ttype, type_records)
            analyses.append(analysis)
        
        # å¤±æ•—å›æ•°ã§ã‚½ãƒ¼ãƒˆ
        analyses.sort(key=lambda a: a.failure_count, reverse=True)
        return analyses
    
    def _analyze_failures(
        self, 
        task_type: str, 
        records: List[TaskExecutionRecord]
    ) -> FailureAnalysis:
        """å¤±æ•—è¨˜éŒ²ã‹ã‚‰åˆ†æã‚’ç”Ÿæˆ"""
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é›†è¨ˆ
        error_counts: Dict[str, int] = {}
        for r in records:
            if r.error_message:
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç°¡ç•¥åŒ–ï¼ˆæœ€åˆã®50æ–‡å­—ï¼‰
                error_key = r.error_message[:50]
                error_counts[error_key] = error_counts.get(error_key, 0) + 1
        
        common_errors = [
            {"message": msg, "count": count}
            for msg, count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        ]
        
        # å¤±æ•—å‰ã®å¹³å‡å®Ÿè¡Œæ™‚é–“
        avg_duration = sum(r.duration_seconds for r in records) / len(records)
        
        # ãƒªã‚½ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        resource_patterns = self._extract_common_resources(records)
        
        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        recommended_actions = self._generate_recommendations(task_type, records, common_errors)
        
        # æœ€çµ‚å¤±æ•—æ—¥æ™‚
        last_failure = max(r.timestamp for r in records)
        
        return FailureAnalysis(
            analysis_id=hashlib.md5(f"{task_type}:failure".encode()).hexdigest()[:12],
            task_type=task_type,
            failure_count=len(records),
            common_errors=common_errors,
            avg_duration_before_failure=avg_duration,
            common_resource_patterns=resource_patterns,
            recommended_actions=recommended_actions,
            last_failure=last_failure
        )
    
    def _generate_recommendations(
        self, 
        task_type: str, 
        records: List[TaskExecutionRecord],
        common_errors: List[Dict[str, Any]]
    ) -> List[str]:
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒå¤šã„å ´åˆ
        timeout_count = sum(1 for r in records if r.result == TaskResult.TIMEOUT)
        if timeout_count > len(records) * 0.3:
            recommendations.append(f"Increase timeout threshold for {task_type} tasks")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„å ´åˆ
        high_memory = sum(1 for r in records if r.resources.get("memory_mb", 0) > 1000)
        if high_memory > len(records) * 0.3:
            recommendations.append(f"Optimize memory usage for {task_type} tasks")
        
        # ãƒªãƒˆãƒ©ã‚¤ãŒå¤šã„å ´åˆ
        high_retry = sum(1 for r in records if r.retry_count > 2)
        if high_retry > len(records) * 0.2:
            recommendations.append(f"Add pre-validation for {task_type} tasks to reduce retries")
        
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«åŸºã¥ãæ¨å¥¨
        for error in common_errors[:3]:
            msg = error["message"].lower()
            if "timeout" in msg or "timed out" in msg:
                recommendations.append("Implement circuit breaker pattern for external API calls")
            elif "memory" in msg or "oom" in msg:
                recommendations.append("Implement chunked processing for large datasets")
            elif "permission" in msg or "unauthorized" in msg:
                recommendations.append("Review and refresh authentication credentials")
            elif "rate limit" in msg:
                recommendations.append("Implement exponential backoff for rate-limited APIs")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å¥¨
        if not recommendations:
            recommendations.append(f"Review {task_type} task implementation for error handling")
        
        return list(set(recommendations))  # é‡è¤‡é™¤å»
    
    def export_learning_data(
        self,
        filepath: Optional[str] = None,
        days_back: int = 90,
        include_patterns: bool = True,
        include_failures: bool = True
    ) -> str:
        """
        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        
        Args:
            filepath: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
            days_back: é¡ã‚‹æ—¥æ•°
            include_patterns: æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚ã‚‹
            include_failures: å¤±æ•—åˆ†æã‚’å«ã‚ã‚‹
        
        Returns:
            å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"data/experience_learning_data_{timestamp}.json"
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(os.path.dirname(filepath) or ".", exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿åé›†
        export_data = {
            "export_info": {
                "exported_at": datetime.now().isoformat(),
                "days_back": days_back,
                "total_records": len(self._local_cache)
            },
            "success_patterns": [],
            "failure_analyses": [],
            "raw_records": []
        }
        
        # æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³
        if include_patterns:
            patterns = self.get_success_patterns(days_back=days_back)
            export_data["success_patterns"] = [p.to_dict() for p in patterns]
        
        # å¤±æ•—åˆ†æ
        if include_failures:
            analyses = self.get_failure_analysis(days_back=days_back)
            export_data["failure_analyses"] = [a.to_dict() for a in analyses]
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹ï¼ˆæœ€è¿‘ã®ã‚‚ã®ã®ã¿ï¼‰
        since = datetime.now() - timedelta(days=days_back)
        recent_records = [r for r in self._local_cache if r.timestamp >= since]
        export_data["raw_records"] = [r.to_dict() for r in recent_records[-100:]]  # æœ€æ–°100ä»¶
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        return filepath
    
    def get_task_statistics(self, days_back: int = 30) -> Dict[str, Any]:
        """
        ã‚¿ã‚¹ã‚¯å®Ÿè¡Œçµ±è¨ˆã‚’å–å¾—
        
        Args:
            days_back: é¡ã‚‹æ—¥æ•°
        
        Returns:
            çµ±è¨ˆãƒ‡ãƒ¼ã‚¿
        """
        since = datetime.now() - timedelta(days=days_back)
        records = self._get_records_in_period(since)
        
        if not records:
            return {
                "period_days": days_back,
                "total_tasks": 0,
                "success_rate": 0.0,
                "avg_duration": 0.0
            }
        
        # åŸºæœ¬çµ±è¨ˆ
        total = len(records)
        successes = sum(1 for r in records if r.result == TaskResult.SUCCESS)
        failures = sum(1 for r in records if r.result == TaskResult.FAILURE)
        timeouts = sum(1 for r in records if r.result == TaskResult.TIMEOUT)
        
        # ã‚¿ã‚¹ã‚¯ç¨®åˆ¥åˆ¥
        by_type: Dict[str, Dict[str, Any]] = {}
        for r in records:
            if r.task_type not in by_type:
                by_type[r.task_type] = {"total": 0, "success": 0, "failure": 0}
            by_type[r.task_type]["total"] += 1
            if r.result == TaskResult.SUCCESS:
                by_type[r.task_type]["success"] += 1
            elif r.result == TaskResult.FAILURE:
                by_type[r.task_type]["failure"] += 1
        
        # æˆåŠŸç‡è¨ˆç®—
        for ttype, stats in by_type.items():
            stats["success_rate"] = stats["success"] / stats["total"] if stats["total"] > 0 else 0.0
        
        return {
            "period_days": days_back,
            "total_tasks": total,
            "success_count": successes,
            "failure_count": failures,
            "timeout_count": timeouts,
            "success_rate": successes / total if total > 0 else 0.0,
            "avg_duration": sum(r.duration_seconds for r in records) / total,
            "by_task_type": by_type,
            "total_retries": sum(r.retry_count for r in records),
            "unique_task_types": len(by_type)
        }
    
    def get_recent_experiences(
        self,
        limit: int = 10,
        result_filter: Optional[TaskResult] = None
    ) -> List[TaskExecutionRecord]:
        """
        æœ€è¿‘ã®çµŒé¨“ã‚’å–å¾—
        
        Args:
            limit: å–å¾—ä»¶æ•°
            result_filter: çµæœã§ãƒ•ã‚£ãƒ«ã‚¿
        
        Returns:
            çµŒé¨“è¨˜éŒ²ãƒªã‚¹ãƒˆ
        """
        records = self._local_cache.copy()
        
        if result_filter:
            records = [r for r in records if r.result == result_filter]
        
        # æ™‚é–“é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        records.sort(key=lambda r: r.timestamp, reverse=True)
        
        return records[:limit]


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_experience_collector_instance: Optional[ExperienceCollector] = None


def get_experience_collector() -> ExperienceCollector:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ExperienceCollectorã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _experience_collector_instance
    if _experience_collector_instance is None:
        _experience_collector_instance = ExperienceCollector()
    return _experience_collector_instance


# ä¾¿åˆ©ãªã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆé–¢æ•°
def record_task(
    task_id: str,
    success: bool,
    duration: float,
    resources: Dict[str, Any],
    task_type: str = "general",
    error: Optional[str] = None,
    **kwargs
) -> str:
    """
    ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’è¨˜éŒ²ã™ã‚‹ç°¡æ˜“é–¢æ•°
    
    Args:
        task_id: ã‚¿ã‚¹ã‚¯ID
        success: æˆåŠŸã—ãŸã‹ã©ã†ã‹
        duration: å®Ÿè¡Œæ™‚é–“ï¼ˆç§’ï¼‰
        resources: ä½¿ç”¨ãƒªã‚½ãƒ¼ã‚¹
        task_type: ã‚¿ã‚¹ã‚¯ç¨®åˆ¥
        error: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆå¤±æ•—æ™‚ï¼‰
        **kwargs: è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    
    Returns:
        è¨˜éŒ²ID
    """
    collector = get_experience_collector()
    result = TaskResult.SUCCESS if success else TaskResult.FAILURE
    
    return collector.record_task_execution(
        task_id=task_id,
        result=result,
        duration=duration,
        resources=resources,
        task_type=task_type,
        error_message=error,
        context=kwargs
    )


def get_learning_insights(task_type: Optional[str] = None) -> Dict[str, Any]:
    """
    å­¦ç¿’ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’å–å¾—ã™ã‚‹ç°¡æ˜“é–¢æ•°
    
    Returns:
        æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã¨å¤±æ•—åˆ†æã®è¦ç´„
    """
    collector = get_experience_collector()
    
    patterns = collector.get_success_patterns(task_type=task_type)
    analyses = collector.get_failure_analysis(task_type=task_type)
    stats = collector.get_task_statistics()
    
    return {
        "success_patterns": [
            {
                "task_type": p.task_type,
                "count": p.success_count,
                "avg_duration": p.avg_duration
            }
            for p in patterns[:5]
        ],
        "top_failure_issues": [
            {
                "task_type": a.task_type,
                "count": a.failure_count,
                "top_error": a.common_errors[0]["message"] if a.common_errors else None
            }
            for a in analyses[:3]
        ],
        "overall_stats": stats
    }


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("ğŸ§  Experience Collector Test")
    
    collector = get_experience_collector()
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
    test_tasks = [
        ("task_001", TaskResult.SUCCESS, 2.5, {"memory_mb": 100, "api_calls": 3}, "code_generation"),
        ("task_002", TaskResult.SUCCESS, 3.0, {"memory_mb": 120, "api_calls": 4}, "code_generation"),
        ("task_003", TaskResult.FAILURE, 5.0, {"memory_mb": 500, "api_calls": 10}, "code_generation", "Timeout error"),
        ("task_004", TaskResult.SUCCESS, 1.5, {"memory_mb": 80, "api_calls": 2}, "data_analysis"),
        ("task_005", TaskResult.SUCCESS, 2.0, {"memory_mb": 90, "api_calls": 2}, "data_analysis"),
    ]
    
    for task_data in test_tasks:
        task_id, result, duration, resources, task_type = task_data[:5]
        error = task_data[5] if len(task_data) > 5 else None
        
        memory_id = collector.record_task_execution(
            task_id=task_id,
            result=result,
            duration=duration,
            resources=resources,
            task_type=task_type,
            error_message=error
        )
        print(f"âœ… Recorded: {task_id} -> {memory_id}")
    
    # æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³å–å¾—
    print("\nğŸ“Š Success Patterns:")
    patterns = collector.get_success_patterns(min_success_count=2)
    for p in patterns:
        print(f"  - {p.task_type}: {p.success_count} successes, avg {p.avg_duration:.2f}s")
    
    # å¤±æ•—åˆ†æå–å¾—
    print("\nğŸ” Failure Analysis:")
    analyses = collector.get_failure_analysis()
    for a in analyses:
        print(f"  - {a.task_type}: {a.failure_count} failures")
        print(f"    Recommended: {a.recommended_actions[0] if a.recommended_actions else 'N/A'}")
    
    # çµ±è¨ˆå–å¾—
    print("\nğŸ“ˆ Statistics:")
    stats = collector.get_task_statistics()
    print(f"  Total tasks: {stats['total_tasks']}")
    print(f"  Success rate: {stats['success_rate']*100:.1f}%")
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
    print("\nğŸ’¾ Exporting learning data...")
    export_path = collector.export_learning_data()
    print(f"  Exported to: {export_path}")
    
    print("\nâœ¨ Test completed!")
