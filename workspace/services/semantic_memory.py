"""Semantic Memory Integration

EntityMemoryã¨semantic.dbã‚’çµ±åˆã—ã€æ„å‘³æ¤œç´¢æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã ã‘ã§ãªãã€æ„å‘³çš„ãªé¡ä¼¼æ€§ã«åŸºã¥ãè¨˜æ†¶æ¤œç´¢ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚
"""

import sqlite3
import os
import json
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import numpy as np

SEMANTIC_DB_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "semantic.db")
MEMORY_DB_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "memory_advanced.db")


class SemanticMemory:
    """æ„å‘³æ¤œç´¢çµ±åˆãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.semantic_db = SEMANTIC_DB_PATH
        self.memory_db = MEMORY_DB_PATH
    
    def search_by_semantic_similarity(
        self,
        query: str,
        top_k: int = 5,
        similarity_threshold: float = 0.5
    ) -> List[Dict[str, Any]]:
        """
        æ„å‘³çš„é¡ä¼¼æ€§ã«åŸºã¥ãè¨˜æ†¶æ¤œç´¢
        
        semantic.dbã®å†…å®¹ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã§è¿‘ä¼¼
        ï¼ˆå°†æ¥çš„ã«ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã‚’çµ±åˆï¼‰
        """
        # ã‚¯ã‚¨ãƒªã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        query_tokens = set(query.lower().split())
        
        conn = sqlite3.connect(self.semantic_db)
        cursor = conn.cursor()
        
        # semantic.dbã®ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’ç¢ºèª
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [row[0] for row in cursor.fetchall()]
        
        results = []
        
        if "documents" in tables:
            # documentsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
            cursor.execute("""
                SELECT id, content, metadata, file_path 
                FROM documents 
                WHERE content LIKE ?
                LIMIT ?
            """, (f"%{query}%", top_k * 2))
            
            for row in cursor.fetchall():
                content = row[1]
                content_tokens = set(content.lower().split())
                
                # ç°¡æ˜“çš„ãªé¡ä¼¼åº¦è¨ˆç®—ï¼ˆJaccardé¡ä¼¼åº¦ï¼‰
                intersection = query_tokens & content_tokens
                union = query_tokens | content_tokens
                similarity = len(intersection) / len(union) if union else 0
                
                if similarity >= similarity_threshold:
                    results.append({
                        "id": row[0],
                        "content": content[:300],
                        "metadata": json.loads(row[2]) if row[2] else {},
                        "source": row[3],
                        "similarity": similarity,
                        "type": "document"
                    })
        
        elif "chunks" in tables:
            # chunksãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
            cursor.execute("""
                SELECT c.id, c.content, d.file_path 
                FROM chunks c
                JOIN documents d ON c.document_id = d.id
                WHERE c.content LIKE ?
                LIMIT ?
            """, (f"%{query}%", top_k * 2))
            
            for row in cursor.fetchall():
                content = row[1]
                content_tokens = set(content.lower().split())
                
                intersection = query_tokens & content_tokens
                union = query_tokens | content_tokens
                similarity = len(intersection) / len(union) if union else 0
                
                if similarity >= similarity_threshold:
                    results.append({
                        "id": row[0],
                        "content": content[:300],
                        "source": row[2],
                        "similarity": similarity,
                        "type": "chunk"
                    })
        
        conn.close()
        
        # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k]
    
    def hybrid_search(
        self,
        query: str,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ + æ„å‘³ï¼‰
        
        EntityMemoryã¨semantic.dbã®ä¸¡æ–¹ã‚’æ¤œç´¢ã—ã€
        çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã™ã‚‹
        """
        from services.entity_memory import get_memory
        
        mem = get_memory()
        
        # 1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆEntityMemoryï¼‰
        keyword_results = mem.recall(query, limit=top_k)
        
        # 2. æ„å‘³æ¤œç´¢ï¼ˆsemantic.dbï¼‰
        semantic_results = self.search_by_semantic_similarity(query, top_k=top_k)
        
        # 3. çµæœã‚’ãƒãƒ¼ã‚¸
        combined = []
        
        # EntityMemoryçµæœã‚’è¿½åŠ 
        for entry in keyword_results:
            combined.append({
                "id": entry.id,
                "content": entry.content,
                "type": entry.memory_type.value,
                "importance": entry.importance.value,
                "source": "memory",
                "tags": entry.tags,
                "score": entry.importance.value * 0.2  # é‡è¦åº¦ã‚’ã‚¹ã‚³ã‚¢ã«åæ˜ 
            })
        
        # Semanticçµæœã‚’è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
        existing_contents = {c["content"][:100] for c in combined}
        for result in semantic_results:
            content_preview = result["content"][:100]
            if content_preview not in existing_contents:
                combined.append({
                    "id": result["id"],
                    "content": result["content"],
                    "type": result["type"],
                    "importance": 3,
                    "source": result.get("source", "unknown"),
                    "similarity": result.get("similarity", 0),
                    "score": result.get("similarity", 0) * 5  # é¡ä¼¼åº¦ã‚’ã‚¹ã‚³ã‚¢ã«
                })
                existing_contents.add(content_preview)
        
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        combined.sort(key=lambda x: x.get("score", 0), reverse=True)
        
        return combined[:top_k]
    
    def index_memory_to_semantic(self, memory_id: str) -> bool:
        """
        EntityMemoryã®è¨˜æ†¶ã‚’semantic.dbã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        ï¼ˆåŒæ–¹å‘æ¤œç´¢ã‚’å¯èƒ½ã«ã™ã‚‹ï¼‰
        """
        from services.entity_memory import get_memory
        
        mem = get_memory()
        entry = mem.get_by_id(memory_id)
        
        if not entry:
            return False
        
        conn = sqlite3.connect(self.semantic_db)
        cursor = conn.cursor()
        
        # documentsãƒ†ãƒ¼ãƒ–ãƒ«ãŒãªã‘ã‚Œã°ä½œæˆ
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS entity_memories (
                id TEXT PRIMARY KEY,
                content TEXT,
                memory_type TEXT,
                tags TEXT,
                importance INTEGER,
                created_at TIMESTAMP,
                metadata TEXT
            )
        """)
        
        # æŒ¿å…¥ã¾ãŸã¯æ›´æ–°
        cursor.execute("""
            INSERT OR REPLACE INTO entity_memories 
            (id, content, memory_type, tags, importance, created_at, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            entry.id,
            entry.content,
            entry.memory_type.value,
            json.dumps(entry.tags),
            entry.importance.value,
            entry.created_at.isoformat(),
            json.dumps(entry.context)
        ))
        
        conn.commit()
        conn.close()
        
        return True
    
    def find_related_context(
        self,
        current_context: str,
        max_results: int = 3
    ) -> str:
        """
        ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’è‡ªå‹•çš„ã«æ¤œç´¢ã—ã€
        LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ„ã¿è¾¼ã‚ã‚‹å½¢å¼ã§è¿”ã™
        """
        results = self.hybrid_search(current_context, top_k=max_results)
        
        if not results:
            return ""
        
        sections = ["## ğŸ” Related Knowledge & Memories\n"]
        
        for i, result in enumerate(results, 1):
            source_icon = "ğŸ§ " if result.get("source") == "memory" else "ğŸ“„"
            sections.append(f"{i}. {source_icon} [{result.get('type', 'unknown')}]")
            sections.append(f"   {result['content'][:250]}...")
            if result.get("tags"):
                sections.append(f"   Tags: {', '.join(result['tags'])}")
            sections.append("")
        
        return "\n".join(sections)


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_semantic_memory_instance: Optional[SemanticMemory] = None


def get_semantic_memory() -> SemanticMemory:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«SemanticMemoryã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _semantic_memory_instance
    if _semantic_memory_instance is None:
        _semantic_memory_instance = SemanticMemory()
    return _semantic_memory_instance


def semantic_search(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """æ„å‘³æ¤œç´¢ã®ç°¡æ˜“é–¢æ•°"""
    sm = get_semantic_memory()
    return sm.search_by_semantic_similarity(query, top_k=top_k)


def hybrid_memory_search(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®ç°¡æ˜“é–¢æ•°"""
    sm = get_semantic_memory()
    return sm.hybrid_search(query, top_k=top_k)


def get_enhanced_context(task_description: str, max_results: int = 3) -> str:
    """
    å¼·åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ï¼ˆsemantic + memoryï¼‰
    """
    sm = get_semantic_memory()
    return sm.find_related_context(task_description, max_results=max_results)


if __name__ == "__main__":
    import sys
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
    
    print("ğŸ” Semantic Memory Test")
    
    sm = get_semantic_memory()
    
    # ãƒ†ã‚¹ãƒˆæ¤œç´¢
    results = sm.hybrid_search("API authentication", top_k=5)
    
    print(f"\nFound {len(results)} results:")
    for r in results:
        print(f"\n[{r.get('type', 'unknown')}] Score: {r.get('score', 0):.2f}")
        print(f"Content: {r['content'][:150]}...")
    
    print("\nâœ… Test completed")
